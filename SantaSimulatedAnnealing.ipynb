{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tingf\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import math\n",
    "from collections import Counter\n",
    "from typing import List, Union\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and device configuration\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "PAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticipantVisibleError(Exception):\n",
    "    \"\"\"Custom exception for participant-visible errors.\"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerplexityCalculator:\n",
    "    def __init__(self, model_path: str, load_in_8bit: bool = False, device_map: str = \"auto\"):\n",
    "        if load_in_8bit and DEVICE.type != \"cuda\":\n",
    "            raise ValueError(\"8-bit quantization requires a CUDA device\")\n",
    "\n",
    "        if load_in_8bit:\n",
    "            quant_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"fp4\",\n",
    "                bnb_4bit_use_double_quant=False,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "            )\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                quantization_config=quant_config,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "        else:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_perplexity(self, input_texts: Union[str, List[str]], batch_size: int = 32) -> Union[float, List[float]]:\n",
    "        is_single = isinstance(input_texts, str)\n",
    "        texts = [input_texts] if is_single else input_texts\n",
    "\n",
    "        losses = []\n",
    "        num_batches = len(texts) // batch_size + (len(texts) % batch_size != 0)\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            batch = texts[i * batch_size: (i + 1) * batch_size]\n",
    "            with torch.no_grad():\n",
    "                tokenized_inputs = self.tokenizer(\n",
    "                    [f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\" for text in batch],\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    add_special_tokens=False,\n",
    "                )\n",
    "                tokenized_inputs = {k: v.to(DEVICE) for k, v in tokenized_inputs.items()}\n",
    "                if \"token_type_ids\" in tokenized_inputs:\n",
    "                    tokenized_inputs.pop(\"token_type_ids\")\n",
    "\n",
    "                outputs = self.model(**tokenized_inputs, use_cache=False)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                labels = tokenized_inputs[\"input_ids\"]\n",
    "                labels[labels == self.tokenizer.pad_token_id] = PAD_TOKEN_LABEL_ID\n",
    "\n",
    "                shift_logits = logits[..., :-1, :].contiguous()\n",
    "                shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "                loss = self.loss_fct(\n",
    "                    shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                    shift_labels.view(-1),\n",
    "                )\n",
    "                loss = loss.view(len(logits), -1)\n",
    "                valid_lengths = (shift_labels != PAD_TOKEN_LABEL_ID).sum(dim=-1)\n",
    "                sequence_losses = torch.sum(loss, -1) / valid_lengths\n",
    "                losses.extend(sequence_losses.cpu().tolist())\n",
    "\n",
    "        return math.exp(losses[0]) if is_single else [math.exp(l) for l in losses]\n",
    "\n",
    "    def clear_gpu_memory(self):\n",
    "        if torch.cuda.is_available():\n",
    "            del self.model\n",
    "            del self.tokenizer\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def score(self, solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str,\n",
    "              model_path: str, load_in_8bit: bool = True, clear_mem: bool = True) -> float:\n",
    "        if not all(solution[row_id_column_name] == submission[row_id_column_name]):\n",
    "            raise ValueError(\"Row IDs in the solution and submission do not match.\")\n",
    "\n",
    "        sol_counts = solution['text'].str.split().apply(Counter)\n",
    "        sub_counts = submission['text'].str.split().apply(Counter)\n",
    "        if not all(sol_counts == sub_counts):\n",
    "            raise ValueError(\"Some submitted strings are not valid permutations of the solution strings.\")\n",
    "\n",
    "        scorer = PerplexityCalculator(model_path=model_path, load_in_8bit=load_in_8bit)\n",
    "        perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n",
    "\n",
    "        if clear_mem:\n",
    "            scorer.clear_gpu_memory()\n",
    "\n",
    "        return float(np.mean(perplexities))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = PerplexityCalculator('/kaggle/input/gemma-2/transformers/gemma-2-9b/2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_start = 10.0\n",
    "temp_end = 0.5\n",
    "cooling_rate = 0.95\n",
    "steps_per_temp = 5\n",
    "def calculate_valid_score(arrangement):\n",
    "    while True:\n",
    "        score = scorer.get_perplexity(arrangement)\n",
    "        if not math.isnan(score):\n",
    "            return score\n",
    "        random.shuffle(arrangement)\n",
    "\n",
    "def simulated_annealing_optimize(\n",
    "        text:str,\n",
    "        temp_start=temp_start,\n",
    "        temp_end=temp_end,\n",
    "        cooling_rate=cooling_rate,\n",
    "        steps_per_temp=steps_per_temp,\n",
    "        verbose=False):\n",
    "\n",
    "    words = text.split()\n",
    "    current_words = words.copy()\n",
    "    current_score = calculate_valid_score(text)\n",
    "    best_score = current_score\n",
    "    best_words = current_words.copy()\n",
    "\n",
    "    temp = temp_start\n",
    "\n",
    "    while temp > temp_end:\n",
    "        for _ in range(steps_per_temp):\n",
    "            i, j = random.sample(range(len(words)), 2)\n",
    "            neighbor = current_words.copy()\n",
    "            neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n",
    "            neighbor_score = scorer.get_perplexity(\" \".join(neighbor))\n",
    "\n",
    "            if math.isnan(neighbor_score):\n",
    "                continue\n",
    "            delta = neighbor_score - current_score\n",
    "            if delta < 0 or random.random() < math.exp(-delta/temp):\n",
    "                current_score = neighbor_score\n",
    "                current_words = neighbor\n",
    "                if current_score < best_score:\n",
    "                    best_score = current_score\n",
    "                    best_words = current_words.copy()\n",
    "\n",
    "        temp *= cooling_rate\n",
    "        if verbose:\n",
    "            print(f\"Temperatur: {temp:.2f}, Current Score: {current_score:.2f}\")\n",
    "\n",
    "    return ' '.join(best_words), best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_csv(\"/kaggle/input/santa-claude-output/submission.csv\")\n",
    "submission = pd.DataFrame(columns=[\"id\", \"text\"])\n",
    "scores = []\n",
    "for i, text in samples.iterrows:\n",
    "    best_words, best_score = simulated_annealing_optimize(text)\n",
    "    scores.append(best_score)\n",
    "    submission.iloc[i] = {\n",
    "        \"id\": i,\n",
    "        \"text\": best_words\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
